{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import urllib\n",
    "from selenium.webdriver.common.by import By\n",
    "from cgitb import text\n",
    "from numpy import append\n",
    "from requests import request\n",
    "from selenium import webdriver\n",
    "\n",
    "class Scraper:\n",
    "\n",
    "    \"\"\"Scrapes driver, team and champions data from the given website.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        URL(str): The URL of the F1 statistics website containing the data\n",
    "        driver: The webdriver used to access the webpage\n",
    "        parent_dir: The parent directory of where the raw data is to be stored\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of nested dictionarys for each driver and team and a dictionary for championship data for each year\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, URL : str, driver : webdriver.Chrome, parent_dir:str):\n",
    "        \n",
    "        self.driver = driver\n",
    "        self.team_list = []\n",
    "        self.URL = URL\n",
    "        self.driver_dict={}\n",
    "        self.teams_dict = {}\n",
    "        self.champs_dict = {}\n",
    "        self.dict_entry = {}\n",
    "        directory = \"raw_data\"\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        if os.path.exists(path) == False:\n",
    "            raw_data = os.mkdir(path)\n",
    "        \n",
    "\n",
    "    def load_and_accept_cookies(self) -> None:\n",
    "\n",
    "        \"\"\"Opens the site and accepts cookies\"\"\"\n",
    "        \n",
    "        self.driver.get(self.URL)\n",
    "        accept_button = self.driver.find_element(by=By.XPATH, value=\"//*[@class='btn btn-success acceptGdpr']\")\n",
    "        accept_button.click()\n",
    "        \n",
    "    def navigate_drivers(self):\n",
    "\n",
    "        \"\"\"Navigates to the list of F1 drivers and calls the get_driver_data method to begin scraping the data for each driver\"\"\"\n",
    "\n",
    "        self.driver.get(self.URL) \n",
    "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Drivers').click()\n",
    "        self.get_driver_data()\n",
    "\n",
    "    def get_image(self):\n",
    "\n",
    "        \"\"\"Method that is called within get_driver_data that locates the element containing the drivers image and calls the function to download the image\"\"\"\n",
    "\n",
    "        img_src = self.driver.find_element(by=By.XPATH, value=\"//img[@class='col-md-3']\").get_attribute('src')\n",
    "        self.download_image(img_src, \"/home/andrew/AICore_work/Data-Collection-Pipeline/driver_images/\", self.get_driver_name())\n",
    "    \n",
    "    def get_URL_list(self,element):\n",
    "\n",
    "        \"\"\"Scrapes URL for each driver\n",
    "        \n",
    "        Args:\n",
    "            element(str):   References the necessary element on the webpage that contains the required data\n",
    "            \n",
    "        Returns:\n",
    "            url_list(list): List of URLS for each driver that can then be looped through to scrape data    \n",
    "        \"\"\"\n",
    "\n",
    "        url_list = []\n",
    "        \n",
    "        starting_letter_tag = self.driver.find_elements(by=By.XPATH, value=element)\n",
    "\n",
    "        for pilot in starting_letter_tag: #collects all the drivers URLS in a list\n",
    "\n",
    "            link = pilot.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "            url_list.append(link)\n",
    "\n",
    "        return url_list\n",
    "        \n",
    "    def download_image(self, url, file_path, file_name):\n",
    "\n",
    "        \"\"\"Downloads the image located in get_image to the required location\n",
    "        \n",
    "        Args:\n",
    "            url:       Webpage containg the required image\n",
    "            file_path: Gives the path of where the image is downloaded to\n",
    "            file_name: Names the file in a Forename_Surname format by calling get_driver_name  \n",
    "        \"\"\"\n",
    "\n",
    "        full_path = file_path + file_name + '.jpg'\n",
    "        urllib.request.urlretrieve(url, full_path)\n",
    "\n",
    "    def stripF1_text(self, tobereplaced):\n",
    "\n",
    "        \"\"\"Reformats the Name of the Driver/Team without any unneccessary text\n",
    "        \n",
    "        Args:\n",
    "            toberreplaced(str):   Unnecessary text to be reformatted\n",
    "            \n",
    "        Returns:\n",
    "            Name(str): Name in required format\n",
    "        \"\"\"\n",
    "        Name = self.driver.find_element(by=By.XPATH, value=\"//h1[@class='page-title']\").text.replace(tobereplaced, \"\")\n",
    "        return Name\n",
    "    \n",
    "    def get_driver_name(self) -> str: \n",
    "\n",
    "        \"\"\"Splits Driver Name string into Forename and Surname and store as variables to be able to add to driver dictionary/use for filename for image\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Name = self.stripF1_text(\"Formula 1 Driver\")\n",
    "        driver_forename = Name.split()[0]\n",
    "        driver_surname = \" \".join(Name.split()[1:len(Name.split())-3])\n",
    "\n",
    "        self.dict_entry[\"Driver First Name\"] = driver_forename\n",
    "        self.dict_entry[\"Driver Second Name\"] = driver_surname\n",
    "        return driver_forename + \"_\" + driver_surname\n",
    "\n",
    "    def get_driver_data(self):\n",
    "\n",
    "        \"\"\"Scrapes data for each driver and stores in a dictionary with a unique reference \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #creates directory for driver data\n",
    "        self.create_dir(\"driver_data\")\n",
    "\n",
    "        #find element containing individual driver URLS\n",
    "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
    "\n",
    "        \n",
    "        for link in url_list: #loops through every URL in the list and scrapes the statistics\n",
    "            \n",
    "            #resets the dictionary entry to blank at the beginning of each URL\n",
    "            self.dict_entry={}\n",
    "\n",
    "            #opens each URL in the list\n",
    "            self.driver.get(link)\n",
    "\n",
    "            self.get_driver_name() #gets the Drivers Name and splits it into First name, Surname\n",
    "            self.get_image()\n",
    "\n",
    "            #scrapes the data from the different columns\n",
    "            column1_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
    "            for i in range(0,len(column1_data),2):\n",
    "                    self.dict_entry[column1_data[i].text] = column1_data[i+1].text\n",
    "            column23_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-3']//td\")\n",
    "            for i in range(0,len(column23_data),2):\n",
    "                    self.dict_entry[column23_data[i].text] = column23_data[i+1].text\n",
    "\n",
    "            #add each entry as a nested dictionary\n",
    "            self.driver_dict[uuid.uuid4().hex] = self.dict_entry\n",
    "\n",
    "        #dump to json file\n",
    "        self.dumptojson(self.driver_dict, \"driver_data.json\")\n",
    "\n",
    "    def navigate_teams(self):\n",
    "\n",
    "        \"\"\"Navigates to the list of F1 teams and calls the get_team_data method to begin scraping the data for each team\"\"\"\n",
    "\n",
    "        self.driver.get(self.URL)\n",
    "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Teams').click()\n",
    "        self.get_team_data()\n",
    "\n",
    "    def get_team_name(self):\n",
    "\n",
    "        \"\"\"Reformats the Name of the Team without any unneccessary text\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Team_Name = self.stripF1_text(\"Formula 1\")\n",
    "        self.dict_entry[\"Team Name\"] = Team_Name\n",
    "        \n",
    "    def get_team_data(self):\n",
    "\n",
    "        \"\"\"Scrapes data for each driver and stores in a dictionary with a unique reference \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        #creates directory for team data\n",
    "        self.create_dir(\"team_data\")\n",
    "\n",
    "        #find element containing individual driver URLS\n",
    "        url_list = self.get_URL_list(\"//div[@class='col-sm-6 col-md-4']\")\n",
    "            \n",
    "        #loops through every URL in the list and scrapes the statistics\n",
    "        for link in url_list[:5]: \n",
    "            \n",
    "            #resets the dictionary entry to blank at the beginning of each URL\n",
    "            self.dict_entry={}\n",
    "\n",
    "            #opens each page in the list of URLs\n",
    "            self.driver.get(link)\n",
    "\n",
    "            #gets the Drivers Name and strips the \"Formula 1\" from it\n",
    "            self.get_team_name() \n",
    "\n",
    "            #scrape the data from the different tables on each page\n",
    "            team_history_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//tbody[@itemtype='http://schema.org/SportsTeam']//td\")\n",
    "            for i in range(0,len(team_history_data),2):\n",
    "                self.dict_entry[team_history_data[i].text] = team_history_data[i+1].text\n",
    "\n",
    "            team_driver_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-6']//td\")\n",
    "            for i in range(0,len(team_driver_data),2):\n",
    "                self.dict_entry[team_driver_data[i].text] = team_driver_data[i+1].text\n",
    "\n",
    "            team_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='col-md-5']//td\")\n",
    "            for i in range(0,len(team_data),2):\n",
    "                self.dict_entry[team_data[i].text] = team_data[i+1].text\n",
    "            \n",
    "            #add each entry as a nested dictionary\n",
    "            self.teams_dict[uuid.uuid4().hex] = self.dict_entry\n",
    "\n",
    "        #dump to json file\n",
    "        self.dumptojson(self.teams_dict, \"teams_data.json\")\n",
    "\n",
    "    def navigate_champs(self):\n",
    "\n",
    "        \"\"\"Navigates to the list of F1 champions and calls the get_champs_data method to begin scraping the data for each championship year\"\"\"\n",
    "\n",
    "        self.driver.get(self.URL)\n",
    "        navbar = self.driver.find_element(by=By.XPATH, value=\"//div[@class='navbar-nav']\").find_element(by=By.LINK_TEXT, value = 'Champions').click()\n",
    "        self.get_champs_data()\n",
    "\n",
    "    def get_champs_data(self):\n",
    "\n",
    "        \"\"\"Scrapes data for each championship year and stores in a dictionary with the year as a unique reference \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #creates directory for driver data\n",
    "        self.create_dir(\"champs_data\")\n",
    "\n",
    "        #find elements that contain champion info\n",
    "        champs_data = self.driver.find_elements(by=By.XPATH, value=\"//div[@class='table-responsive']//td\")\n",
    "\n",
    "        #loop through elements to separate into data by year\n",
    "        for i in range(0,len(champs_data),4):\n",
    "                self.dict_entry={}\n",
    "                self.dict_entry[\"Year\"] = champs_data[i].text\n",
    "                self.dict_entry[\"Driver\"] = champs_data[i+1].text\n",
    "                self.dict_entry[\"Driver's Team\"] = champs_data[i+2].text\n",
    "                self.dict_entry[\"Winning Team\"] = champs_data[i+3].text\n",
    "                self.champs_dict[uuid.uuid4().hex] = self.dict_entry\n",
    "                \n",
    "        #dump to json file\n",
    "        self.dumptojson(self.champs_dict, \"champs_data.json\")\n",
    "\n",
    "    def create_dir(self,directory):\n",
    "        \n",
    "        \"\"\"Checks if the path/folder to be created already exists and if not, creates the directory\n",
    "\n",
    "        Args:\n",
    "            directory: name of the file to be created\n",
    "\n",
    "        \"\"\"\n",
    "        parent_dir = \"/home/andrew/AICore_work/Data-Collection-Pipeline/raw_data\"\n",
    "        path = os.path.join(parent_dir, directory)\n",
    "        if os.path.exists(path) == False:\n",
    "            raw_data = os.mkdir(path)\n",
    "\n",
    "    def dumptojson(self, dictionary, out_file):\n",
    "\n",
    "        \"\"\"Dumps the collected information into a named file.json\n",
    "\n",
    "\n",
    "        Args:\n",
    "            dictionary: name of the dictionary to be dumped to json file\n",
    "            out_file: name of the file in which the dictionary is to be placed\n",
    "\n",
    "        \"\"\"\n",
    "        out_file = open(out_file, \"w\")\n",
    "        json.dump(dictionary, out_file, indent = 6)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = Scraper(\"https://www.4mula1stats.com/\", webdriver.Chrome(), \"/home/andrew/AICore_work/Data-Collection-Pipeline\")\n",
    "    scraper.load_and_accept_cookies()\n",
    "    scraper.navigate_drivers()\n",
    "    scraper.navigate_teams()\n",
    "    scraper.navigate_champs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa77068f38439fea3d523770773ba61bd6d72ba10bc6cca53fd2c1ada3d3a952"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
